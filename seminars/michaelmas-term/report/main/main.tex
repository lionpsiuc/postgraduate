\documentclass[12pt]{article}

\input{../preamble/preamble.tex}

\title{Report on \textit{Attention Is All You Need}\\\vspace{0.5cm}\large Submitted in Partial Fulfillment of the Requirements for Seminars During the Hilary Term}
\author{Ion Lipsiuc}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}\noindent This report provides an in-depth analysis of the influential paper \textit{Attention Is All You Need},\footnote{See \citep{vaswani2017attention}.} which introduces the transformer model - a groundbreaking architecture that relies entirely on attention mechanisms, thereby eliminating the need for recurrence. We detail the model's core components, including self-attention, multi-head attention, and positional encodings, and review experimental results that demonstrate its significant performance improvements on machine translation tasks.\end{abstract}

\tableofcontents
\newpage

\input{../sections/introduction/introduction.tex}
\input{../sections/background-and-related-work/background-and-related-work.tex}
\input{../sections/transformer-architecture/transformer-architecture.tex}
\input{../sections/experimental-setup-and-results/experimental-setup-and-results.tex}
\input{../sections/discussion/discussion.tex}
\input{../sections/conclusion/conclusion.tex}

\bibliographystyle{plainnat}
\bibliography{../references}

\end{document}
