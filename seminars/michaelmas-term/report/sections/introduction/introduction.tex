\section{Introduction}

Recent advances in natural language processing (NLP) have been driven by the need to efficiently handle sequential data in tasks such as machine translation. Traditional recurrent neural networks (RNNs), while effective, struggle with long-range dependencies and sequential processing limitations. These constraints prompted the search for more parallelisable approaches.

In \textit{Attention Is All You Need}, the authors introduce the transformer - an architecture that replaces recurrence with self-attention mechanisms. Unlike RNNs, the transformer attends to all input positions simultaneously, assigning dynamic weights to tokens and capturing global dependencies regardless of distance. Its multi-head attention allows the model to focus on different aspects of the sequence concurrently, while positional encodings maintain sequence order information.

This design significantly improves computational efficiency through parallelisation while achieving state-of-the-art performance on translation tasks. The transformer's impact extends beyond machine translation, establishing a foundation for numerous subsequent NLP advancements.

This report analyses the transformer architecture, examining its core components - self-attention, multi-head attention, and positional encodings - and their contribution to the model's performance. We explore the experimental results and discuss the transformative influence this work has had.
