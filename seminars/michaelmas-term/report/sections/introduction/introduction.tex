\section{Introduction}

Recent advances in natural language processing (NLP) have been driven by the need to improve the way models handle sequential data, especially in tasks such as machine translation. Traditionally, recurrent neural networks (RNNs) and their variants, although effective, have encountered limitations in capturing long-range dependencies due to their sequential processing nature. These limitations have motivated researchers to explore more parallelizable and efficient approaches.

In their seminal work, \textit{Attention Is All You Need}, the authors introduce the transformer - a novel architecture that completely abandons recurrence in favour of a self-attention mechanism. Unlike RNN-based models, the transformer is capable of attending to all parts of an input sequence simultaneously. This self-attention mechanism assigns dynamic weights to each token in the sequence, allowing the model to capture global dependencies regardless of the distance between words.

A key innovation of the transformer is its use of multi-head attention. By employing multiple attention heads, the model can focus on different parts of the sequence concurrently, which enhances its ability to learn complex relationships within the data. Additionally, the incorporation of positional encodings ensures that the model retains information about the order of the sequence, compensating for the absence of recurrence.

The architectural design of the transformer not only addresses the inherent limitations of RNNs but also significantly improves computational efficiency. With its high degree of parallelisation, the transformer reduces training time while achieving state-of-the-art performance on machine translation benchmarks. This breakthrough has not only advanced the field of machine translation but has also paved the way for numerous subsequent developments in NLP.

In this report, we provide an in-depth analysis of the transformer architecture as presented in \textit{Attention Is All You Need}. We will examine its core components - including self-attention, multi-head attention, and positional encodings - and discuss how these innovations contribute to its superior performance. Through a detailed exploration of the experimental results and the model's overall impact, we aim to highlight the transformative influence of this work on modern NLP research.
