{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Studies in High-Performance Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 - Communication-Avoiding Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import any necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import qr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we set a global printing setting below where we only print up to four decimal places in order to make the results easier to visualise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How the Implementation Follows the Mathematical Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below implements the TSQR factorisation as described in `README.md`. Below is a step-by-step breakdown of how the code follows the theory:\n",
    "\n",
    "1. **Matrix Partitioning**: The function extracts the matrix dimensions `m` and `n`, then determines the `blockSize = m // 4` to divide the matrix into four row blocks. It then iterates over four blocks using a `for` loop and slices the corresponding rows from `W`.\n",
    "2. **Local QR Factorisation**: Each block `W_i` undergoes a local QR factorisation using `qr(W[rows, :], mode=\"economic\")`. The computed `Q_i` (orthogonal) and `R_i` (upper triangular) are stored in `Q_blocks` and `R_blocks`, respectively.\n",
    "3. **Reduction Step**: The local `R_i` matrices (each of size $n\\times n$) are stacked vertically using `np.vstack(R_blocks)`, forming a $4n\\times n$ matrix. Another QR factorisation is performed on this reduced matrix: `Q2, R_final = qr(R_stacked, mode=\"economic\")`. Here, `Q2` is an intermediate orthogonal matrix of size $4n\\times n$, and `R_final` is the final upper triangular matrix $n\\times n$.\n",
    "4. **Constructing the Final $Q$**: A zero matrix `Q` of size $m\\times n$ is initialised to store the final orthogonal factor. The intermediate `Q2` is partitioned into four sub-matrices corresponding to the four processors (theoretically speaking, since this code merely simulates the communication-avoiding TSQR factorisation by performing computations in sequential steps), and each `Q_i` is updated as `Q_blocks[i] @ Q2_i`. This step combines the local orthogonal matrices `Q_i` with the global reduction matrix `Q2`, resulting in the final `Q`.\n",
    "5. **Verification**: A random matrix `A` of size $16\\times4$ is used for testing. We perform the factorisation, and then the reconstructed matrix `Q @ R` is compared to `A`. The residual norm, given by `A - Q @ R` (i.e., $||A-QR_{\\text{final}}||$ is calculated) is printed to check the accuracy of the factorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix, A:\n",
      "[[-1.3595  0.9003 -0.3804  0.314 ]\n",
      " [-0.3389  0.5992  0.1897  0.6584]\n",
      " [ 0.0507  0.9738 -2.0994 -1.5068]\n",
      " [-1.0978 -1.6898  0.2815 -2.1273]\n",
      " [ 0.1333  1.3869  0.9983  0.9044]\n",
      " [ 0.5232  1.3155  0.2982  1.1079]\n",
      " [ 0.3749 -0.7843  0.5366 -0.8139]\n",
      " [-0.414  -1.8155 -0.5376  0.5128]\n",
      " [ 0.762   0.3767 -0.3396 -0.7472]\n",
      " [-1.0872 -0.7293 -2.1003 -0.0609]\n",
      " [ 1.1346 -0.8348  0.1458  0.8893]\n",
      " [ 0.3075 -0.1048 -0.3756  1.5401]\n",
      " [ 0.9048 -0.2402 -0.1158 -1.2824]\n",
      " [ 0.9794 -0.1732 -0.2978 -0.6528]\n",
      " [-1.1376 -0.4185 -1.1667 -0.0106]\n",
      " [-0.524  -0.0697  1.2095 -3.4765]]\n",
      "\n",
      "Final orthogonal matrix, Q:\n",
      "[[ 0.4243  0.315   0.009   0.0639]\n",
      " [ 0.1058  0.1795 -0.071   0.1152]\n",
      " [-0.0158  0.2596  0.6025 -0.4197]\n",
      " [ 0.3426 -0.3966 -0.1938 -0.251 ]\n",
      " [-0.0416  0.3664 -0.245   0.1149]\n",
      " [-0.1633  0.3264 -0.0199  0.1193]\n",
      " [-0.117  -0.2313 -0.1307 -0.1152]\n",
      " [ 0.1292 -0.4669  0.0875  0.2177]\n",
      " [-0.2378  0.0608  0.1622 -0.2184]\n",
      " [ 0.3393 -0.1384  0.4838  0.0266]\n",
      " [-0.3541 -0.2854  0.0389  0.1733]\n",
      " [-0.096  -0.0447  0.1278  0.2829]\n",
      " [-0.2824 -0.113   0.1021 -0.2864]\n",
      " [-0.3057 -0.0989  0.1598 -0.1757]\n",
      " [ 0.3551 -0.052   0.2251  0.0471]\n",
      " [ 0.1636  0.0092 -0.3798 -0.6179]]\n",
      "\n",
      "Final upper triangular matrix, R:\n",
      "[[-3.2039 -0.6349 -0.9743 -0.8756]\n",
      " [ 0.      3.7128  0.2072  1.1298]\n",
      " [ 0.      0.     -3.599   0.5316]\n",
      " [ 0.      0.      0.      5.0848]]\n",
      "\n",
      "Reconstructed matrix, QR:\n",
      "[[-1.3595  0.9003 -0.3804  0.314 ]\n",
      " [-0.3389  0.5992  0.1897  0.6584]\n",
      " [ 0.0507  0.9738 -2.0994 -1.5068]\n",
      " [-1.0978 -1.6898  0.2815 -2.1273]\n",
      " [ 0.1333  1.3869  0.9983  0.9044]\n",
      " [ 0.5232  1.3155  0.2982  1.1079]\n",
      " [ 0.3749 -0.7843  0.5366 -0.8139]\n",
      " [-0.414  -1.8155 -0.5376  0.5128]\n",
      " [ 0.762   0.3767 -0.3396 -0.7472]\n",
      " [-1.0872 -0.7293 -2.1003 -0.0609]\n",
      " [ 1.1346 -0.8348  0.1458  0.8893]\n",
      " [ 0.3075 -0.1048 -0.3756  1.5401]\n",
      " [ 0.9048 -0.2402 -0.1158 -1.2824]\n",
      " [ 0.9794 -0.1732 -0.2978 -0.6528]\n",
      " [-1.1376 -0.4185 -1.1667 -0.0106]\n",
      " [-0.524  -0.0697  1.2095 -3.4765]]\n",
      "\n",
      "Residual norm, ||A - QR|| = 2.078329765560058e-15\n"
     ]
    }
   ],
   "source": [
    "def tsqr(W):\n",
    "    \"\"\"Compute the tall-skinny QR (TSQR) factorisation of a matrix.\n",
    "\n",
    "    Args:\n",
    "        W (numpy.ndarray): A NumPy array (of size m x n), representing the matrix to factorise, where m >> n (i.e., the matrix is tall and skinny).\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - Q (numpy.ndarray): A NumPy array (of size m x n), representing the orthogonal (i.e., Q factor of the decomposition) matrix.\n",
    "            - R (numpy.ndarray): A NumPy array (of size n x n), representing the upper triangular (i.e., R factor of the decomposition) matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract matrix dimensions\n",
    "    m, n = W.shape\n",
    "\n",
    "    # How many rows each block should have\n",
    "    blockSize = m // 4\n",
    "\n",
    "    # Initialise storage for local decompositions\n",
    "    Q_blocks = []\n",
    "    R_blocks = []\n",
    "\n",
    "    # Local QR factorisation loop\n",
    "    for i in range(4):\n",
    "\n",
    "        # Extract out the relevant block for the iteration\n",
    "        rows = slice(i * blockSize, (i + 1) * blockSize)\n",
    "\n",
    "        # Carry out local QR factorisation\n",
    "        Q_i, R_i = qr(W[rows, :], mode=\"economic\")\n",
    "\n",
    "        # Store results\n",
    "        Q_blocks.append(Q_i)\n",
    "        R_blocks.append(R_i)\n",
    "\n",
    "    # Stack the upper triangular matrices vertically\n",
    "    R_stacked = np.vstack(R_blocks)\n",
    "\n",
    "    # QR factorisation on the reduced matrix above\n",
    "    Q2, R_final = qr(R_stacked, mode=\"economic\")\n",
    "\n",
    "    # Initialise a matrix to store final orthogonal matrix\n",
    "    Q = np.zeros((m, n))\n",
    "\n",
    "    # Loop to carry out the multiplication to obtain the final orthogonal matrix\n",
    "    for i in range(4):\n",
    "\n",
    "        # Extracting out the relevant blocks\n",
    "        Q2_i = Q2[i * n : (i + 1) * n, :]\n",
    "\n",
    "        # Multiplication of old blocks with these new ones\n",
    "        Q[i * blockSize : (i + 1) * blockSize, :] = Q_blocks[i] @ Q2_i\n",
    "\n",
    "    return Q, R_final\n",
    "\n",
    "\n",
    "# Test\n",
    "m, n = 16, 4\n",
    "A = np.random.randn(m, n)\n",
    "Q, R = tsqr(A)\n",
    "A_reconstructed = Q @ R\n",
    "residual = np.linalg.norm(A - A_reconstructed)\n",
    "print(\"Original matrix, A:\")\n",
    "print(A)\n",
    "print(\"\\nFinal orthogonal matrix, Q:\")\n",
    "print(Q)\n",
    "print(\"\\nFinal upper triangular matrix, R:\")\n",
    "print(R)\n",
    "print(\"\\nReconstructed matrix, QR:\")\n",
    "print(A_reconstructed)\n",
    "print(f\"\\nResidual norm, ||A - QR|| = {residual}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cases",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
